"""Backend supported: tensorflow.compat.v1, tensorflow, pytorch"""
import os
import deepxde.deepxde as dde
import numpy as np
import random
# Backend pytorch
import torch
import matplotlib.pyplot as plt
from torch import nn
from skopt.space.space import Integer, Real
from skopt import gp_minimize
import os.path

T = 0.1
Z = 560e-3
ad0 = 6.636388907942602e-06 * T / Z ** 2
bd0 = 0.0001384336110920574 * T / Z ** 2
td0 = 415.887409390378
ac0 = 19.086175786524592
bc0 = 146.4990242134754
tc0 = 1088.804568376853
Tmax = 2000 +273.15#K
Tmin = 25 +273.15#Â°C
Umax = (Tmax)*(ac0 +bc0/(1+ (Tmax)/tc0))
Umin = (Tmin)*(ac0 +bc0/(1+ (Tmin)/tc0))
# size of the tile
height = 1#5e-3
width = 1#28e-3
tile_thick = 28e-3
tile_maxwidth = Z
tile_minwidth = 100e-3
mu = width/2
sigma = 1e-1
Time_size = 1
Z = tile_maxwidth
T = 0.1
alpha_net = (ad0+bd0) 
xlim = tile_thick / tile_minwidth

## Domain par
ND = 10000# Nx*Ny*Nt
Nx = np.round((ND/alpha_net*xlim**3/width/Time_size)**(1/4))#20#
Ny = np.round(Nx*width/xlim)
Nt = max([np.round(Nx**2*alpha_net*Time_size/xlim**2), 1])
Nsx = int(Ny*Nt)#1000
Nsy = int(Nx*Nt)#2000
Nst = int(Nx*Ny)#100

num_inputs = 3
## Domain par
resample=10000
ND = 10000# Nx*Ny*Nt
Nx = np.round((ND/alpha_net*xlim**3/width/Time_size)**(1/4))#20#
Ny = np.round(Nx*width/xlim)
Nt = max([np.round(Nx**2*alpha_net*Time_size/xlim**2), 1])
Nsx = int(Ny*Nt)#1000
Nsy = int(Nx*Nt)#2000
Nst = int(Nx*Ny)#100

num_test = 10000
## Network parameters

loss = 'MSE'
optim = "adam"
## weights
weights = [1, 1, 1, 1, 1]
#weights = np.ndarray.tolist(np.exp(weights)/sum(np.exp(weights)))
#weights = [0, 0, 0, 1]
loss_names=['PDE_loss', 'BC_x loss','BC_y1 loss','BC_y2 loss','IC loss']
best_loss = 1e3
total_neurons = 64
#opt folder
fold = 'alphau_opt'

def pde_matprop(x, u):
    du_t = dde.grad.jacobian(u, x, i=0, j=2)
    du_xx = dde.grad.hessian(u, x, i=0, j=0)
    du_yy = dde.grad.hessian(u, x, i=1, j=1)
    alpha = alpha_mat(ad0, bd0, td0, u)
    # Backend pytorch
    return du_t - alpha * (du_xx+du_yy)


def alpha_mat(ad0, bd0, td0, u):
    t = u_to_t(u, ac0, bc0, tc0)
    return ad0 + bd0/(1+t/td0)**2


def u_to_t(u, ac0, bc0, tc0):
    b = (ac0 + bc0) * tc0 - u
    c = u*tc0
    return (torch.sqrt(b**2+4*ac0*c) - b) / (2*ac0)


def t_to_u(t, ac0, bc0, tc0):
    return t*(ac0 +bc0/(1+t/tc0))



def boundary_x(x, on_boundary):
    return on_boundary & np.isclose(x[0], 0)#qui xlim


def boundary_y1(x, on_boundary):
    return (on_boundary & np.isclose(x[1], 0) & (x[0] > 0.) & (x[0] < xlim))


def boundary_y2(x, on_boundary):
    return (on_boundary & np.isclose(x[1], width) & (x[0] > 0.) & (x[0] < xlim))


def func_x(x):
    return (Umax-Umin)*np.exp(-(mu-x[:, 1:2])**2/(2*sigma**2))+Umin


def func_y(x):
    return x[:, 1:2]*0


def func_IC(x):
    step = np.heaviside(-x[:,0:1],1)
    return step*np.exp(-(mu-x[:, 1:2])**2/(2*sigma**2))*(Umax-Umin)+Umin


def create_model(X):
    layers = X[0]
    lr = X[1]

    ##Fix seed
    seed = 155
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.enabled = False
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if total_neurons%layers:
        neurons = int(total_neurons/layers)
        extra_neurons = total_neurons%layers
        if layers%2:
            layer_size = [3] + [neurons] * int(layers/2) + [neurons+extra_neurons] + [neurons] * int(layers/2) + [1]
        else:
            layer_size = [3] + [neurons] * int(layers/2-1) + [int(neurons+extra_neurons/2)] * 2 +\
                         [neurons] * int(layers/2-1) + [1]
    else:
        neurons = int(total_neurons/layers)
        layer_size = [3] + [neurons] * int(layers) + [1]
    print(layer_size)
    activation = "tanh"
    initializer = "Glorot uniform"
    net = dde.maps.FNN(layer_size, activation, initializer)
    net = dde.maps.FNN(layer_size, activation, initializer)

    if net.linears[-1].bias is not None:
        nn.init.constant_(net.linears[-1].bias.data, Umin/Umax+1e-4)
    net.apply_output_transform(lambda x,y:y*Umax)
    ## mesh geometry
    geom = dde.geometry.Rectangle([0, 0], [xlim, width])
    ## time space
    timedomain = dde.geometry.TimeDomain(0, Time_size)
    geomtime = dde.geometry.GeometryXTime(geom, timedomain)
    ic = dde.IC(geomtime, func_IC, lambda _, on_initial: on_initial)
    # bc1 = dde.DirichletBC(geomtime, func, lambda _, on_boundary: )
    bc_x = dde.DirichletBC(geomtime, func_x, boundary_x, component=0, num_points=Nsx)  # Nsx)
    bc_y1 = dde.NeumannBC(geomtime, func_y, boundary_y1, component=0, num_points=Nsy)  # )
    bc_y2 = dde.NeumannBC(geomtime, func_y, boundary_y2, component=0, num_points=Nsy)  # )
    data = dde.data.TimePDE(
        geomtime,
        pde_matprop,
        [bc_x, bc_y1, bc_y2, ic],
        num_domain=ND,  # ND
        num_boundary=(Nsx + Nsy) * 4,  # Nx+Ny
        num_initial=Nst,  # Nt
        num_test=num_test,
        train_distribution="pseudo",
        seed=seed,
        #   exclusions=
    )
    # show_train_points(data)
    
    model = dde.Model(data, net)
    model.compile(optim, lr=lr, loss=loss, loss_weights=weights,loss_norm=Umax**2)
    return model

def fitness(X):
    """
    Hyper-parameters:
    learning_rate:     Learning-rate for the optimizer.
    num_dense_layers:  Number of dense layers.
    num_dense_nodes:   Number of nodes in each dense layer.
    """
    num_dense_layers = X[0]
    lr = X[1]
    # Print the hyper-parameters.
    print('num_dense_layers:', num_dense_layers)
    print('learning rate:', lr)
    print()

    # Create the neural network with these hyper-parameters.
    model = create_model(X)
    # Dir-name for the TensorBoard log-files.
    logname = 'SKOPT_theo_BC_weak__alphau_{}_layers_lr_{lr:.1e}'.format(num_dense_layers,lr=lr)
    if not(os.path.isfile(fold+'/' + logname + '.txt')):
        resampler = dde.callbacks.PDEResidualResampler(period=10000)
        early_stop = dde.callbacks.EarlyStopping(min_delta=1e-8, patience=50000)
        losshistory, train_state = model.train(epochs=500000, display_every=500, save_every=500,
                                               model_save_path=fold+'/'+logname+'.pt',
                                               logname=logname,
                                               callbacks=[resampler, early_stop],
                                               Tensorboard=True, loss_names=loss_names)

        # Get the classification accuracy on the validation-set
        # after the last training-epoch.
        loss = np.nanmin(np.sum(losshistory.loss_test,axis=1))
        global best_loss

        # Save the model history to hard disk.
        dde.save_loss_history(losshistory, fold+'/' + logname + '.txt')
        # Update the classification loss.
        if loss<best_loss:
            best_loss = loss

    else:
        with open(fold+'/' + logname + '.txt') as f:
            lines = f.readlines()
        lines_number = [i.split() for i in lines[1:]]
        loss = np.array([np.double(i) for i in lines_number])
        loss = np.nanmin(np.sum(loss[:, 5:], axis=1))

    # Print the classification accuracy.
    print()
    print("Loss: {0:.2e}".format(loss))
    print()




    # NOTE: Scikit-optimize does minimization so it tries to
    # find a set of hyper-parameters with the LOWEST fitness-value.
    # Because we are interested in the HIGHEST classification
    # loss, we need to negate this number so it can be minimized.
    return loss


# This function exactly comes from :Hvass-Labs, TensorFlow-Tutorials


dim_num_dense_layers = Integer(low=2, high=8, name='num_dense_layers')
dim_lr = Real(low=1e-6, high=1e-2, name='learning rate')
dimensions = [dim_num_dense_layers,
              dim_lr]
default_parameters = [2,3e-4]


search_result = gp_minimize(func=fitness,
                            dimensions=dimensions,
                            acq_func='EI', # Expected Improvement.
                            n_calls=40,
                            x0=default_parameters)
e = 3+1

e = 1

